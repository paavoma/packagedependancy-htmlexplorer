<!DOCTYPE html>
<html>
<head>
<link rel="stylesheet" type="text/css" href="defaultStyle.css">
</head>
<body>
<div class="right">
<h3><a href='index.html'>index</a></h3>
<h1>wget</h1>
<h2>Depends: </h2>
<ol>
<li><a href='libc6.html'>libc6</a></li>
<li><a href='libidn11.html'>libidn11</a></li>
<li><a href='libssl1_0_0.html'>libssl1.0.0</a></li>
<li><a href='dpkg.html'>dpkg</a></li>
<li> install-info</li>
</ol>
<h2>Reverse dependencies: </h2>
<ol>
<li><a href='ubuntu-standard.html'>ubuntu-standard</a></li>
<li><a href='ssh-import-id.html'>ssh-import-id</a></li>
</ol>
<h2>Description: </h2>
<p>retrieves files from the web</p>
<p> Wget is a network utility to retrieve files from the web</p>
<p> using HTTP(S) and FTP, the two most widely used internet</p>
<p> protocols. It works non-interactively, so it will work in</p>
<p> the background, after having logged off. The program supports</p>
<p> recursive retrieval of web-authoring pages as well as FTP</p>
<p> sites -- you can use Wget to make mirrors of archives and</p>
<p> home pages or to travel the web like a WWW robot.</p>
<p> .</p>
<p> Wget works particularly well with slow or unstable connections</p>
<p> by continuing to retrieve a document until the document is fully</p>
<p> downloaded. Re-getting files from where it left off works on</p>
<p> servers (both HTTP and FTP) that support it. Both HTTP and FTP</p>
<p> retrievals can be time stamped, so Wget can see if the remote</p>
<p> file has changed since the last retrieval and automatically</p>
<p> retrieve the new version if it has.</p>
<p> .</p>
<p> Wget supports proxy servers; this can lighten the network load,</p>
<p> speed up retrieval, and provide access behind firewalls.</p>
</div>
</body>
</html>
