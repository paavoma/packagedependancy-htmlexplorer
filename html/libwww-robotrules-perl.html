<!DOCTYPE html>
<html>
<head>
<link rel="stylesheet" type="text/css" href="defaultStyle.css">
</head>
<body>
<div class="right">
<h3><a href='index.html'>index</a></h3>
<h1>libwww-robotrules-perl</h1>
<h2>Depends: </h2>
<ol>
<li><a href='perl.html'>perl</a></li>
<li><a href='liburi-perl.html'>liburi-perl</a></li>
</ol>
<h2>Reverse dependencies: </h2>
<ol>
<li><a href='libwww-perl.html'>libwww-perl</a></li>
</ol>
<h2>Description: </h2>
<p>database of robots.txt-derived permissions</p>
<p> WWW::RobotRules parses /robots.txt files as specified in "A Standard for</p>
<p> Robot Exclusion", at <http://www.robotstxt.org/wc/norobots.html>. Webmasters</p>
<p> can use the /robots.txt file to forbid conforming robots from accessing parts</p>
<p> of their web site.</p>
<p> .</p>
<p> The parsed files are kept in a WWW::RobotRules object, and this object</p>
<p> provides methods to check if access to a given URL is prohibited. The same</p>
<p> WWW::RobotRules object can be used for one or more parsed /robots.txt files</p>
<p> on any number of hosts.</p>
</div>
</body>
</html>
